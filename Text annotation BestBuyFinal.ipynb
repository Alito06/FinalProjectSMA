{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Social Media Analytics\n",
    "### Introduction to Text Mining\n",
    "## Text Annotation\n",
    " Best Buy by Eva"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk \n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtypes = {\n",
    "    'device': 'category',\n",
    "    'user': 'category',\n",
    "    'rating': 'float64',\n",
    "    'text': 'string',\n",
    "    'data': 'category',\n",
    "    'ownership_length': 'category',\n",
    "}\n",
    "\n",
    "ds = pd.read_excel(\n",
    "    \"ExtractedReviewsDataCollection_bestbuy.xlsx\",\n",
    "    dtype=dtypes,\n",
    " )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text preprocessing\n",
    "def textPreProcess(rawText, removeHTML=True, charsToRemove = r'\\?|\\.|\\!|\\;|\\.|\\\"|\\,|\\(|\\)|\\&|\\:|\\-', removeNumbers=True, removeLineBreaks=False, specialCharsToRemove = r'[^\\x00-\\xfd]', convertToLower=True, removeConsecutiveSpaces=True):\n",
    "    if type(rawText) != str:\n",
    "        return rawText\n",
    "    procText = rawText\n",
    "        \n",
    "    # Remove HTML\n",
    "    if removeHTML:\n",
    "        procText = BeautifulSoup(procText,'html.parser').get_text()\n",
    "\n",
    "    # Remove punctuation and other special characters\n",
    "    if len(charsToRemove)>0:\n",
    "        procText = re.sub(charsToRemove,' ',procText)\n",
    "\n",
    "    # Remove numbers\n",
    "    if removeNumbers:\n",
    "        procText = re.sub(r'\\d+',' ',procText)\n",
    "\n",
    "    # Remove line breaks\n",
    "    if removeLineBreaks:\n",
    "        procText = procText.replace('\\n',' ').replace('\\r', '')\n",
    "\n",
    "    # Remove special characters\n",
    "    if len(specialCharsToRemove)>0:\n",
    "        procText = re.sub(specialCharsToRemove,' ',procText)\n",
    "\n",
    "    # Normalize to lower case\n",
    "    if convertToLower:\n",
    "        procText = procText.lower() \n",
    "\n",
    "    # Replace multiple consecutive spaces with just one space\n",
    "    if removeConsecutiveSpaces:\n",
    "        procText = re.sub(' +', ' ', procText)\n",
    "\n",
    "    return procText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize words\n",
    "def tokenize_words(words):\n",
    "    if (type(words) != str) or (word_tokenize(words) == ''):\n",
    "        return np.nan\n",
    "    else:\n",
    "        return word_tokenize(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Adventure.4.Eva.r\\anaconda3\\lib\\site-packages\\bs4\\__init__.py:435: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Create a dataframe with only the description\n",
    "processedReviews =  pd.DataFrame(data=ds.text.apply(textPreProcess).values, index=ds.index, columns=['PreProcessedText']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize text\n",
    "processedReviews['Words'] =  processedReviews['PreProcessedText'].apply(tokenize_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RangeIndex(start=0, stop=374, step=1)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processedReviews.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "374\n"
     ]
    }
   ],
   "source": [
    "print(len(processedReviews.Words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Adventure.4.Eva.r\\AppData\\Roaming\\nltk_data..\n",
      "[nltk_data]     .\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('phone', 'NN'), ('is', 'VBZ'), ('fine', 'JJ'), ('purchase', 'NN'), ('process', 'NN'), ('was', 'VBD'), ('a', 'DT'), ('disaster', 'NN'), ('salesman', 'NN'), ('was', 'VBD'), ('distracted', 'VBN'), ('interrupted', 'VBN'), ('by', 'IN'), ('former', 'JJ'), ('clients', 'NNS'), ('and', 'CC'), ('other', 'JJ'), ('sales', 'NNS'), ('people', 'NNS'), ('never', 'RB'), ('had', 'VBD'), ('his', 'PRP$'), ('full', 'JJ'), ('attention', 'NN'), ('he', 'PRP'), ('apparently', 'RB'), ('entered', 'VBD'), ('erroneous', 'JJ'), ('information', 'NN'), ('now', 'RB'), ('leading', 'VBG'), ('to', 'TO'), ('a', 'DT'), ('follow', 'JJ'), ('up', 'RP'), ('visit', 'NN'), ('at', 'IN'), ('a', 'DT'), ('different', 'JJ'), ('store', 'NN'), ('never', 'RB'), ('again', 'RB')]\n"
     ]
    }
   ],
   "source": [
    "tags = nltk.pos_tag(processedReviews.Words.iloc[37])\n",
    "print(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['phone', 'purchase', 'process', 'disaster', 'salesman', 'clients', 'sales', 'people', 'attention', 'information', 'visit', 'store']\n"
     ]
    }
   ],
   "source": [
    "# Filter only Nouns\n",
    "nouns = []\n",
    "for tag in tags:\n",
    "    if tag[1][0]==\"N\":  # if if starts with a \"N\"\n",
    "        nouns.append(tag[0])\n",
    "print(nouns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adjectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fine', 'former', 'other', 'full', 'erroneous', 'follow', 'different']\n"
     ]
    }
   ],
   "source": [
    "adjectives = []\n",
    "for tag in tags:\n",
    "    if tag[1][0]==\"J\":  # if it starts with a \"J\"\n",
    "        adjectives.append(tag[0])\n",
    "print(adjectives)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### verbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['is', 'was', 'was', 'distracted', 'interrupted', 'had', 'entered', 'leading']\n"
     ]
    }
   ],
   "source": [
    "verbs = []\n",
    "for tag in tags:\n",
    "    if tag[1][0] == \"V\":  # if it starts with a \"V\"\n",
    "        verbs.append(tag[0])\n",
    "print(verbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d7a18bc08bf6b314cad8b0dd8f53415ad78d1015cc806d14e4873c235fb4e191"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
